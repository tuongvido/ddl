"""
Test PhoBERT to find which words/phrases are classified as TOXIC (LABEL_1)
"""
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def test_toxic_words():
    """Test many Vietnamese words to see which ones are classified as toxic"""
    
    print("=" * 80)
    print("PHOBERT TOXIC WORDS DETECTION")
    print("=" * 80)
    
    # Load model
    model_path = Path(__file__).parent.parent / "models" / "phobert"
    print(f"\n‚è≥ Loading model from: {model_path}")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(str(model_path))
    model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
    model.to(device)
    model.eval()
    
    print(f"‚úÖ Model loaded (Device: {device})")
    print(f"   Labels: {model.config.id2label}")
    
    # List of Vietnamese words/phrases to test
    test_words = [
        # Potentially toxic words
        "ngu", "ƒëi√™n", "ch√≥", "kh·ªën", "ƒë·ªì", "m·∫π", "b·ªë", 
        "ƒë·ª•", "ƒë√©o", "ƒë·ªì ngu", "th·∫±ng ngu", "con ch√≥",
        "m√†y", "tao", "ƒëm", "vcl", "ƒëcm", "cc", "lol",
        "ƒë·ªì kh·ªën", "kh·ªën n·∫°n", "ch·∫øt ti·ªát", "ƒë·ªì ch√≥",
        "ƒë·ªì l·ª´a ƒë·∫£o", "ngu ng·ªëc", "ng·ªõ ng·∫©n", "d·ªët",
        "con l·ª£n", "con heo", "ƒë·ªì ƒëi√™n", "ƒëi√™n kh√πng",
        "c√∫t ƒëi", "bi·∫øn ƒëi", "c√¢m mi·ªáng", "im m·ªìm",
        "ƒë√°nh nhau", "gi·∫øt", "ch·∫øt", "ƒë·∫≠p ch·∫øt",
        
        # Non-toxic words
        "xin ch√†o", "c·∫£m ∆°n", "vui", "ƒë·∫πp", "t·ªët",
        "y√™u", "th∆∞∆°ng", "h·∫°nh ph√∫c", "vui v·∫ª",
        "b·∫°n", "anh", "ch·ªã", "em", "t√¥i",
        "l√†m vi·ªác", "h·ªçc t·∫≠p", "ƒÉn c∆°m", "ng·ªß ngh·ªâ",
        "th·ªùi ti·∫øt", "ƒë·∫πp tr·ªùi", "m∆∞a", "n·∫Øng",
    ]
    
    print(f"\nüîç Testing {len(test_words)} words/phrases:")
    print("=" * 80)
    
    toxic_words = []
    clean_words = []
    
    for word in test_words:
        try:
            # Tokenize
            inputs = tokenizer(
                word,
                return_tensors="pt",
                truncation=True,
                max_length=256,
                padding=True
            ).to(device)
            
            # Inference
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=-1)
                predicted_class = torch.argmax(probs, dim=-1).item()
                confidence = probs[0][predicted_class].item()
            
            # Store results
            if predicted_class == 1:  # LABEL_1 = toxic
                toxic_words.append({
                    'word': word,
                    'confidence': confidence,
                    'toxic_prob': probs[0][1].item()
                })
            else:  # LABEL_0 = clean
                clean_words.append({
                    'word': word,
                    'confidence': confidence
                })
        except Exception as e:
            print(f"Error testing '{word}': {e}")
    
    # Print TOXIC words
    print("\n" + "=" * 80)
    print(f"üö® TOXIC WORDS (LABEL_1): {len(toxic_words)}")
    print("=" * 80)
    
    # Sort by confidence
    toxic_words.sort(key=lambda x: x['toxic_prob'], reverse=True)
    
    for i, item in enumerate(toxic_words, 1):
        print(f"{i:2d}. '{item['word']:<20}' ‚Üí Toxic: {item['toxic_prob']:>6.2%}")
    
    # Print CLEAN words
    print("\n" + "=" * 80)
    print(f"‚úÖ CLEAN WORDS (LABEL_0): {len(clean_words)}")
    print("=" * 80)
    
    for i, item in enumerate(clean_words[:20], 1):  # Show first 20
        print(f"{i:2d}. '{item['word']}'")
    
    if len(clean_words) > 20:
        print(f"... and {len(clean_words) - 20} more clean words")
    
    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    print(f"Total tested: {len(test_words)}")
    print(f"Toxic (LABEL_1): {len(toxic_words)}")
    print(f"Clean (LABEL_0): {len(clean_words)}")
    print("=" * 80)

if __name__ == "__main__":
    test_toxic_words()
