import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Trá» Ä‘áº¿n thÆ° má»¥c chá»©a 5 file vá»«a táº£i vá»
MODEL_PATH = Path(__file__).parent.parent / "models" / "phobert"

print("Äang load model...")
try:
    # Load model tá»« thÆ° má»¥c offline
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_PATH, local_files_only=True
    )

    print("Load thÃ nh cÃ´ng! Sáºµn sÃ ng kiá»ƒm tra.")

    # HÃ m dá»± Ä‘oÃ¡n
    def predict(text):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
        with torch.no_grad():
            logits = model(**inputs).logits
        probs = torch.nn.functional.softmax(logits, dim=-1)
        pred_label = torch.argmax(probs).item()

        return "Äá»˜C Háº I ğŸ¤¬" if pred_label == 1 else "BÃŒNH THÆ¯á»œNG ğŸ˜Š"

    # Test thá»­
    while True:
        text = input("\nNháº­p cÃ¢u bÃ¬nh luáº­n: ")
        if text == "exit":
            break
        print(f"Káº¿t quáº£: {predict(text)}")

except Exception as e:
    print(f"Lá»—i: {e}")
    print(
        "Báº¡n hÃ£y kiá»ƒm tra xem Ä‘Ã£ táº£i Ä‘á»§ 5 file (Ä‘áº·c biá»‡t lÃ  model.safetensors) vÃ o Ä‘Ãºng thÆ° má»¥c chÆ°a nhÃ©."
    )
